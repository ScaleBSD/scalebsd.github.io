many scalability problems lie not in the implementation, but in the design of the software interface. By the time developers have an implementation, a workload, and the hardware to demonstrate a bottleneck, interface-level solutions may be impractical or impossible.

whenever interface operations commute, they can be implemented in a way that scales.
This rule makes intuitive sense: when operations commute, their results (return values and effect on system state) are independent of order. Hence, communication between commutative operations is unnecessary, and eliminating it yields a conflict-free implementation. On modern shared-memory multicores, conflict-free operations can execute entirely from per-core caches, so the performance of a conflict-free implementation will scale linearly with the number of cores.


The scalable commutativity rule leads to a new way to design scalable software: analyze the interface’s commutativity; if possible, refine or redesign the interface to improve its commutativity; and then design an implementation that scales when operations commute.


 Commuter takes a symbolic interface model, computes precise conditions under which sets of operations commute, generates concrete tests of commutative operations, and uses these tests to reveal conflicts in an implementation. Any conflicts Commuter finds represent an opportunity for the developer to improve the scalability of their implementation. This tool can be integrated into the software develop- ment process to drive initial design and implementation, to incrementally improve existing implementations, and to help developers understand the commutativity of an interface.

From this model, Commuter generates 26,238 tests of commutative system call pairs, all of which can be made conflict-free according to the rule. Applying this suite to Linux, we find that the Linux kernel is conflict-free for 17,206 (65%) of these cases. Many of the commutative cases where Linux is not conflict-free are important to applications—such as commutative mmaps and creating different files in a shared directory—and reflect bottlenecks found in previous work [11


Israeli and Rappoport introduce the notion of disjoint-access-parallel memory systems [41]. Roughly, if a shared memory system is disjoint-access-parallel and a set of processes access disjoint memory locations, then those processes scale linearly. Like the commutativity rule, this is a conditional scalability guarantee: if the application uses shared memory in a particular way, then the shared memory implementation will scale. However, where disjoint-access parallelism is specialized to the memory system interface, our work encompasses any software interface. Attiya et al. extend Israeli and Rappoport’s definition to additionally require non- disjoint reads to scale [3]. Our work builds on the assumption that memory systems behave this way, and we confirm that real hardware closely approximates this behavior (chapter 3).
Both the original disjoint-access parallelism paper and subsequent work, including the paper by Roy et al. [56], explore the scalability of processes that have some amount of non- disjoint sharing, such as compare-and-swap instructions on a shared cache line or a shared lock. Our work takes a black-and-white view because we have found that, on real hardware, a single modified shared cache line can wreck scalability (chapters 3 and 9).

Practitioners often follow an iterative process to improve scalability: design, implement, mea- sure, repeat [14]. Through a great deal of effort, this approach has led kernels such as Linux to scale well for many important workloads. However, Linux still has many scalability bottle- necks, and absent a method for reasoning about interface-level scalability, it is unclear which of the bottlenecks are inherent to its system call interface. This dissertation identifies situations where POSIX permits or limits scalability and points out specific interface modifications that would permit greater implementation scalability.


Prior work on concolic testing [33, 58] and symbolic execution [12, 13] generates test cases by symbolically executing a specific implementation. Our Commuter tool uses a combination of symbolic and concolic execution, but generates test cases for an arbitrary implementation based on a model of that implementation’s interface. This resembles QuickCheck’s [15] or Gast’s [42] model-based testing, but uses symbolic techniques. Furthermore, while symbolic execution systems often avoid reasoning precisely about symbolic memory accesses (e.g., accessing a symbolic offset in an array), Commuter’s test case generation aims to achieve conflict coverage (section 6.2), which tests different access patterns when using symbolic addresses or indexes.


Designing commutative interfaces:
Many POSIX APIs combine several operations into one, limiting the combined operation’s commutativity. For example, fork both creates a new process and snapshots the current process’s entire memory state, file descriptor state, signal mask, and several other properties. As a result, fork fails to commute with most other operations in the same process, such as memory writes, address space operations, and many file descriptor operations. However, applications often follow fork with exec, which undoes most of fork’s sub-operations. With only fork and exec, applications are forced to accept these unnecessary sub-operations that limit commutativity.

Another example, stat, retrieves and returns many different attributes of a file simul- taneously, which makes it non-commutative with operations on the same file that change any attribute returned by stat (such as link, chmod, chown, write, and even read). In practice, applications invoke stat for just one or two of the returned fields.

5.2 Embrace specification non-determinism
POSIX’s “lowest available FD” rule is a classic example of overly deterministic design that results in poor scalability. Because of this rule, open operations in the same process (and any other FD allocating operations) do not commute, since the order in which they execute determines the returned FDs. This constraint is rarely needed by applications and an alternate interface that could return any unused FD would allow FD allocation operations to commute and enable implementations to use well-known scalable allocation methods. We will return to this example, too, in section 9.2. Many other POSIX interfaces get this right: mmap can return any unused virtual address and creat can assign any unused inode number to a new file.
5.3 Permit weak ordering
Another common source of limited commutativity is strict ordering requirements between operations. For many operations, ordering is natural and keeps interfaces simple to use; for example, when one thread writes data to a file, other threads can immediately read that data. Synchronizing operations like this are naturally non-commutative. Communication interfaces, on the other hand, often enforce strict ordering, but may not need to. For instance, most systems order all messages sent via a local Unix domain socket, even when using SOCK_DGRAM, so any send and recv system calls on the same socket do not commute (except in error conditions). This is often unnecessary, especially in multi-reader or multi-writer situations, and an alternate interface that does not enforce ordering would allow send and recv to commute as long as there is both enough free space and enough pending messages on the socket, which would in turn allow an implementation of Unix domain sockets to support scalable communication (which we use in section 9.3).
5.4 Release resources asynchronously
A closely related problem is that many POSIX operations have global effects that must be visible before the operation returns. This is generally good design for usable interfaces, but for operations that release resources, this is often stricter than applications need and expensive to ensure. For example, writing to a pipe must deliver SIGPIPE immediately if there are no



Figure 7-1: Conflict-freedom of commutative system call pairs in Linux, showing the fraction and absolute number of test cases generated by Commuter that are not conflict-free for each system call pair. One example test case was shown in Figure 6-6.


